{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac4d0b0d",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AlexandruSamoila/trend_analysis/blob/main/src/finetune_roberta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366065cb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fLwfKfTYQIVq",
    "outputId": "41d561d5-7e34-4524-b628-74fab12e4bf8"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbbf6e1",
   "metadata": {
    "id": "W0ZgkxVCK8_c"
   },
   "outputs": [],
   "source": [
    "# Importing the libraries needed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import transformers\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe0310f",
   "metadata": {
    "id": "V_q1ztT4LDEh"
   },
   "outputs": [],
   "source": [
    "# Setting up the device for GPU usage\n",
    "\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad96c6d9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lj2EYCDVUHiH",
    "outputId": "c654537a-e753-4adc-d6b7-7e10d49d47f8"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dda881",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8WChDl_sVBLg",
    "outputId": "00c76f2f-d3da-47e7-8878-79cfa9cee3e3"
   },
   "outputs": [],
   "source": [
    "%cd drive/MyDrive/Projects/trend_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f6e3e7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tl7kbRYBVgaf",
    "outputId": "55372dff-70fd-4529-b047-f3857e062f0b"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a223bd02",
   "metadata": {
    "id": "ayFy1gkXVMXB"
   },
   "outputs": [],
   "source": [
    "posts_df = pd.read_json(\"data/analyzed_trends.json\", encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d38e3a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "kvjianhfVTCw",
    "outputId": "aa74b0fd-bcd1-4af5-d793-e09619575c18"
   },
   "outputs": [],
   "source": [
    "posts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526d291f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3qqqSbTFT1sz",
    "outputId": "1d0a2a95-7837-4b5e-93a2-ac8cf5f6de48"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class_counts = Counter(posts_df['label'])\n",
    "print(\"Before balancing:\", class_counts)\n",
    "\n",
    "# Define a proportion for balancing\n",
    "negative_size = class_counts[0]  # Keep all negatives\n",
    "neutral_size = int(negative_size * 1.3)  # Keep 1.3x more neutrals\n",
    "positive_size = int(negative_size * 1.1)  # Keep 1.1x more positives\n",
    "\n",
    "balanced_df = pd.DataFrame()\n",
    "\n",
    "# Sample negative class (keep all)\n",
    "neg_df = posts_df[posts_df['label'] == 0]\n",
    "\n",
    "# Sample neutral class (keep more)\n",
    "neu_df = posts_df[posts_df['label'] == 1].sample(n=min(neutral_size, class_counts[1]), random_state=42)\n",
    "\n",
    "# Sample positive class (undersample a bit)\n",
    "pos_df = posts_df[posts_df['label'] == 2].sample(n=min(positive_size, class_counts[2]), random_state=42)\n",
    "\n",
    "# Combine into balanced dataset\n",
    "balanced_df = pd.concat([neg_df, neu_df, pos_df])\n",
    "\n",
    "print(\"After balancing:\", Counter(balanced_df['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972056bc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269,
     "referenced_widgets": [
      "81a2784ee83f4f6c9fb0fc4b5ed4d610",
      "cfd4cf498b9e448782a125e7d93069fd",
      "dfda5fbc923d41b4ae76574edab4ab05",
      "87cf86d7790045f394ad9f2964b55963",
      "62a7001821fe4a198cd3b4e05d72d6de",
      "a38fe9a6d2e44c6bb1db05bfdcf0df4b",
      "4797289427954ea5b7a7f7f0ed55bf84",
      "0d832e435492469abfa92057402038e9",
      "e8700c94f9864f0c93da53bc4dfa1aa2",
      "c71091b91c3849e4bf1f028e12eabefc",
      "27560d7930334d7dba66c8deb5e12ede",
      "6dd1517771cc4bfda44a4a3f5e6e254d",
      "db1eb62e4bc6468086167efa67cd0264",
      "7440044fa0f94319a148857d84ce935c",
      "4dbbc44c2ece4b788cf2811e3c031497",
      "b7c69af1c7b14878904b09a60357cc24",
      "71ad0689637f4f31873196386fc67282",
      "109a47aa08e64f62a96577a47286c08c",
      "3474f474288342e78ad8c4b75748f998",
      "5fa5217fc36841849d34ac2554530c81",
      "f86be2d0995e4fc88659118b1a504831",
      "064c4e63f1334adbac82de938b71304a",
      "68615c2f021d4de59187a335b353e9dc",
      "54fb2918cd5c45408e24cfebde2095fb",
      "ca1539d7ac664bf292f184052dfcb8c7",
      "c9f8dc641fb143bba9ef2c4863400437",
      "1dd31f3e4f5a4c99a9f6fd392745f16e",
      "7489449eea8d4908af617cf60a986734",
      "8ad5aaca5412498fb88adafa627e4d1c",
      "27dac352ab1544e6b91a018b0e2cb621",
      "698ba94879b342b3b5b9e21bd1d9f8b1",
      "5062c227cdb742a9a9d549e55238fc6f",
      "e7c4672a7de640189e955f91676334a0",
      "796a2300dd2743a0b0b0545e17261603",
      "830b34c736f247418760b8bd655e059d",
      "76b7635171c54bc3b5c2577c11ed262c",
      "c63326fe372941c4b6d58dec02536e42",
      "72f33dceb0494734a6d5b5f4e446f745",
      "5567b941d4d449518332109a017023f8",
      "ddd79c89b94d44cc8c21ff4957469e35",
      "4b5ebb75b0624d66bd55032d59a0c65e",
      "03b1dd748ec843458b469b2c21bb0c54",
      "a76e8126e93b45b3abc168f19d6dcd07",
      "51a5a48e60014fa1bed1226da575c8e0"
     ]
    },
    "id": "sRHrAE_8Yp0f",
    "outputId": "910af546-8e1c-4d5e-a848-29842a1fb4d0"
   },
   "outputs": [],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 215\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 8\n",
    "TEST_BATCH_SIZE = 8\n",
    "# EPOCHS = 1\n",
    "LEARNING_RATE = 1e-05\n",
    "model_type = \"cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual\" #clapAI/modernBERT-large-multilingual-sentiment\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type, truncation=True, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815c2884",
   "metadata": {
    "id": "X44K-GfbYqZg"
   },
   "outputs": [],
   "source": [
    "class SentimentData(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.post\n",
    "        self.targets = self.data.label\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get the actual row using iloc to avoid index issues\n",
    "        row = self.data.iloc[index]  # Use iloc to access by position\n",
    "        text = str(row.post)\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(row.label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575ebc34",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2KI_Q0fiYOkJ",
    "outputId": "1c78cd4a-5dbf-4a9d-bee1-5159cfecf0f1"
   },
   "outputs": [],
   "source": [
    "train, validate, test = np.split(\n",
    "        balanced_df.sample(frac=1, random_state=42),\n",
    "        [int(0.8 * len(balanced_df)), int(0.9 * len(balanced_df))],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2787482d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8_DZE85UZU0L",
    "outputId": "6915ec5c-44b9-4295-cab3-9f73dea7e41b"
   },
   "outputs": [],
   "source": [
    "print(\"Train, Validate, Test:\",train.shape, validate.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde2c454",
   "metadata": {
    "id": "QxFLwKtZZnZQ"
   },
   "outputs": [],
   "source": [
    "training_set = SentimentData(train, tokenizer, MAX_LEN)\n",
    "validation_set = SentimentData(validate, tokenizer, MAX_LEN)\n",
    "test_set = SentimentData(test, tokenizer, MAX_LEN)\n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 1,\n",
    "                'pin_memory' : False\n",
    "                }\n",
    "valid_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 1,\n",
    "                'pin_memory' : False\n",
    "                }\n",
    "test_params = {'batch_size': TEST_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 1,\n",
    "                 'pin_memory' : False\n",
    "                }\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "validation_loader = DataLoader(validation_set, **valid_params)\n",
    "test_loader = DataLoader(test_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0cd2ab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S9w4sVGobJYB",
    "outputId": "d42e5bac-4423-4dc8-de9f-678a24002a2f"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_type)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3968c34f",
   "metadata": {
    "id": "Olh84y2aOX9H"
   },
   "outputs": [],
   "source": [
    "def calcuate_accuracy(preds, targets):\n",
    "    n_correct = (preds==targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657863e0",
   "metadata": {
    "id": "I1jvCagVcPgU"
   },
   "outputs": [],
   "source": [
    "\n",
    "def one_epoch(model, data_loader, loss_fn, opt=None):\n",
    "    \"\"\"\n",
    "    Runs one epoch of training or validation.\n",
    "\n",
    "    Args:\n",
    "        model: The model to be trained or evaluated.\n",
    "        data_loader: DataLoader object that loads the batch of data.\n",
    "        loss_fn: The loss function to optimize.\n",
    "        opt: Optimizer for training phase (None during evaluation).\n",
    "\n",
    "    Returns:\n",
    "        avg_loss: Average loss over the epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the model to training or evaluation mode based on the optimizer\n",
    "    train = False if opt is None else True\n",
    "    model.train() if train else model.eval()\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "\n",
    "    losses = []\n",
    "    for _,data in enumerate(data_loader):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "        # Enable gradient computation only during training\n",
    "        with torch.set_grad_enabled(train):\n",
    "            outputs = model(ids, mask, token_type_ids).logits\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        tr_loss += loss.item()\n",
    "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "        n_correct += calcuate_accuracy(big_idx, targets)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=targets.size(0)\n",
    "\n",
    "        # if _%5000==0:\n",
    "        #     loss_step = tr_loss/nb_tr_steps\n",
    "        #     accu_step = (n_correct*100)/nb_tr_examples\n",
    "        #     print(f\"Training Loss per 5000 steps: {loss_step}\")\n",
    "        #     print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n",
    "\n",
    "        if train:\n",
    "            opt.zero_grad()  # Reset gradients\n",
    "            loss.backward()  # Backpropagate gradients\n",
    "            opt.step()  # Update model parameters\n",
    "            # if scheduler:\n",
    "            #     scheduler.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    avg_loss = np.mean(losses)\n",
    "    # print(f'The Total Accuracy for Epoch: {(n_correct*100)/nb_tr_examples}')\n",
    "    # epoch_loss = tr_loss/nb_tr_steps\n",
    "    # epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    # print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    # print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "    return avg_loss\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    lr=1e-3,\n",
    "    max_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    patience=3,\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains the model using the provided training and validation data loaders.\n",
    "\n",
    "    Args:\n",
    "        model: The neural network model to train.\n",
    "        train_loader: DataLoader object for the training set.\n",
    "        val_loader: DataLoader object for the validation set.\n",
    "        lr: Learning rate for the optimizer. Default is 1e-3.\n",
    "        max_epochs: Maximum number of epochs to train. Default is 30.\n",
    "        weight_decay: Weight decay for L2 regularization. Default is 0.01.\n",
    "        patience: Number of epochs with no improvement after which training will stop. Default is 3.\n",
    "\n",
    "    Returns:\n",
    "        train_losses: List of training losses per epoch.\n",
    "        valid_losses: List of validation losses per epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    best_valid_loss = float(\"inf\")  # Initialize best validation loss\n",
    "    patience_counter = 0  # Counter for early stopping\n",
    "    train_losses, valid_losses = [], []  # Track losses for plotting/analysis\n",
    "    best_model_state = None  # Store the best model's state_dict\n",
    "\n",
    "    # Training loop\n",
    "    t = tqdm(range(max_epochs))\n",
    "    for epoch in t:\n",
    "        # Training phase\n",
    "        train_loss = one_epoch(model, train_loader, loss_fn, opt)\n",
    "        # Validation phase\n",
    "        valid_loss = one_epoch(model, val_loader, loss_fn)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "\n",
    "        # Early stopping\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            patience_counter = 0  # Reset counter if validation loss improves\n",
    "            best_model_state = model.state_dict() # Save best model\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\nEarly stopping at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "        t.set_description(f\"train loss: {train_loss:.4f}, val loss: {valid_loss:.4f}\")\n",
    "    # Load the best model state before returning\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return train_losses, valid_losses\n",
    "\n",
    "def plot_history(train_losses, valid_losses):\n",
    "    \"\"\"\n",
    "    Plots the training and validation loss history over epochs.\n",
    "\n",
    "    Args:\n",
    "        train_losses: List of training losses for each epoch.\n",
    "        valid_losses: List of validation losses for each epoch.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(7, 3))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.plot(train_losses, label=\"train\")\n",
    "    plt.plot(valid_losses, label=\"valid\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c64da3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "id": "eAsh3RrBibkv",
    "outputId": "63cb0e88-8b03-4d31-ac50-124e3253a316"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 3\n",
    "PATIENCE = 2\n",
    "WEIGHT_DECAY = 0.02\n",
    "\n",
    "# Start the training process and plot the training history\n",
    "print(\"Begin training ...\")\n",
    "plot_history(\n",
    "    *train(\n",
    "        model,\n",
    "        training_loader,\n",
    "        validation_loader,\n",
    "        lr=LEARNING_RATE,\n",
    "        max_epochs=EPOCHS,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        patience=PATIENCE,\n",
    "    )\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df2a7e5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PWZGk9Ewspx9",
    "outputId": "dfb9ad66-cb2a-4685-b376-530218c4e615"
   },
   "outputs": [],
   "source": [
    "# Save the trained model and its hyperparameters\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "# Folder name to save the trained model\n",
    "models_dir = \"checkpoints\"\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Create model name with hyperparameters\n",
    "model_name = \"roberta_sentiment\"\n",
    "\n",
    "# Construct the path for saving the model\n",
    "save_dir = os.path.join(models_dir, f\"{timestamp}_{model_name}\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "model_path = os.path.join(save_dir, \"model.bin\")\n",
    "model.save_pretrained(save_dir)\n",
    "# torch.save(model.state_dict(), model_path)\n",
    "tokenizer.save_vocabulary(save_dir)\n",
    "print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f95ecc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Al8tHlWEfi-s",
    "outputId": "5058e53f-1975-4b3d-a624-c04169c48855"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "del model\n",
    "torch.cuda.empty_cache()  # Free GPU memory\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc027ea5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jbkeFuMPwWR7",
    "outputId": "f7868ca4-2e1f-4f0f-c264-1f6547e95a8a"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14533da",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e31D9VsfuI1D",
    "outputId": "4f1ac936-f7ed-4b24-c3a2-7d415700b9ef"
   },
   "outputs": [],
   "source": [
    "test_model = AutoModelForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/Projects/trend_analysis/checkpoints/20250407_1450_roberta_sentiment\")\n",
    "test_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109d626f",
   "metadata": {
    "id": "nuSjt_-dyfNz"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "def evaluate_model(model, data_loader):\n",
    "    n_correct = 0\n",
    "    nb_tr_examples = 0\n",
    "    all_predictions = []\n",
    "    all_true_ratings = []\n",
    "    class_counts = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for _,data in enumerate(data_loader):\n",
    "          ids = data['ids'].to(device, dtype = torch.long)\n",
    "          mask = data['mask'].to(device, dtype = torch.long)\n",
    "          token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "          targets = data['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "          outputs = model(ids, mask, token_type_ids).logits\n",
    "\n",
    "          _, predictions = torch.max(outputs.data, dim=1)\n",
    "          # Store for confusion matrix\n",
    "          all_predictions.extend(predictions.cpu().numpy())\n",
    "          all_true_ratings.extend(targets.cpu().numpy())\n",
    "          n_correct += calcuate_accuracy(predictions, targets)\n",
    "          nb_tr_examples+=targets.size(0)\n",
    "          # Update class-specific counts\n",
    "          for true_label, pred_label in zip(targets.cpu().numpy(), predictions.cpu().numpy()):\n",
    "              class_counts[true_label][\"total\"] += 1\n",
    "              if true_label == pred_label:\n",
    "                  class_counts[true_label][\"correct\"] += 1\n",
    "    # Print results for each class\n",
    "    print(\"\\nPerformance per class:\")\n",
    "    for sentiment_class, counts in sorted(class_counts.items()):\n",
    "        correct = counts[\"correct\"]\n",
    "        total = counts[\"total\"]\n",
    "        print(f\"Class {sentiment_class}: {correct}/{total} correct ({(correct/total)*100:.2f}%)\")\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(all_true_ratings, all_predictions, normalize='true')\n",
    "    class_labels = sorted(class_counts.keys())  # Ensure correct class ordering\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "    print(classification_report(all_true_ratings, all_predictions, digits=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8aa987",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 747
    },
    "id": "wqs7bU93z_WH",
    "outputId": "3199621e-1c23-4621-b7a4-f5d5164cbb2a"
   },
   "outputs": [],
   "source": [
    "test_model.to(device)\n",
    "evaluate_model(test_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c94623b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 765
    },
    "id": "z_iC5EgO0wPj",
    "outputId": "bb2d0ff8-a042-4fa4-ccec-7c93498bb79d"
   },
   "outputs": [],
   "source": [
    "init_model = AutoModelForSequenceClassification.from_pretrained(model_type)#\"cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual\")\n",
    "init_model.to(device)\n",
    "init_model.eval()\n",
    "print(evaluate_model(init_model, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd71342",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ASwGX1_YCCgd",
    "outputId": "fc0b3d09-8f60-4e78-b2de-9c67b23563c8"
   },
   "outputs": [],
   "source": [
    "del init_model\n",
    "del test_model\n",
    "torch.cuda.empty_cache()  # Free GPU memory\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
