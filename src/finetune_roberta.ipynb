{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyN/tC/l6eZ1twYKRmOFukgs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"46a298f602854d1fb1c381c689a5fa27":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0bc2aae051ff41229193bd6ddc359644","IPY_MODEL_b317ac03cd9545c2b5f860dab3d3e5e8","IPY_MODEL_20dcea837a1a4cd898f50491e836e958"],"layout":"IPY_MODEL_50e8e19a68d24d6eaecbbe4ca83c7c65"}},"0bc2aae051ff41229193bd6ddc359644":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b79787e63854401486086d1b6adbaaaf","placeholder":"​","style":"IPY_MODEL_08c602c3897041b795702e3f4bd798a3","value":"tokenizer_config.json: 100%"}},"b317ac03cd9545c2b5f860dab3d3e5e8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_354842de0ab443e8828ff975a99f72f2","max":443,"min":0,"orientation":"horizontal","style":"IPY_MODEL_324d0c6787e04ce0ae0012e45ce0488e","value":443}},"20dcea837a1a4cd898f50491e836e958":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4789e69093f047da8b59e8f3f83e95b6","placeholder":"​","style":"IPY_MODEL_00ea35acadad44afadd2835ffff03824","value":" 443/443 [00:00&lt;00:00, 15.8kB/s]"}},"50e8e19a68d24d6eaecbbe4ca83c7c65":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b79787e63854401486086d1b6adbaaaf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08c602c3897041b795702e3f4bd798a3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"354842de0ab443e8828ff975a99f72f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"324d0c6787e04ce0ae0012e45ce0488e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4789e69093f047da8b59e8f3f83e95b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"00ea35acadad44afadd2835ffff03824":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ed56f740eb4041afa20a9bb0cfb92062":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_168795b00a8e4ee9b9c54aeba51ce5f9","IPY_MODEL_c9e4e7c345d547ca81f79dd115a4a504","IPY_MODEL_dc916c794cbf49ce8e9dae0c4d4313ce"],"layout":"IPY_MODEL_ea3858e9182a498fbef0532e2d0e7bac"}},"168795b00a8e4ee9b9c54aeba51ce5f9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_92d485a2fa254dcdb5061d6da735877d","placeholder":"​","style":"IPY_MODEL_a5c17baae3414ac58a8698b2219500a1","value":"sentencepiece.bpe.model: 100%"}},"c9e4e7c345d547ca81f79dd115a4a504":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2906efda23874955b1043ce0ada1c2c9","max":5069051,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ffbee74e38af43b0938a2792f8fd136e","value":5069051}},"dc916c794cbf49ce8e9dae0c4d4313ce":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1fd38e10747447538d74735ab53b61ce","placeholder":"​","style":"IPY_MODEL_f97d5fcb240349f788b646078c0aaae4","value":" 5.07M/5.07M [00:00&lt;00:00, 20.1MB/s]"}},"ea3858e9182a498fbef0532e2d0e7bac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92d485a2fa254dcdb5061d6da735877d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5c17baae3414ac58a8698b2219500a1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2906efda23874955b1043ce0ada1c2c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ffbee74e38af43b0938a2792f8fd136e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1fd38e10747447538d74735ab53b61ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f97d5fcb240349f788b646078c0aaae4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bd430ce15d3f4a63b35eed4ff1cc8ed9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3b69f42a75cc481ebdc63626c81f596b","IPY_MODEL_7e35f6ce168b4749a0e6639d5d2db430","IPY_MODEL_a35e89be7b8145c6b960d84d550da75b"],"layout":"IPY_MODEL_9693faa63a9442eeb4af842f4502bf7d"}},"3b69f42a75cc481ebdc63626c81f596b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b41886a6e61e40e586d223304070d1b9","placeholder":"​","style":"IPY_MODEL_1f02c65cbdd84d14b12966ed421420e4","value":"tokenizer.json: 100%"}},"7e35f6ce168b4749a0e6639d5d2db430":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c76dad8a674b491ca2e6168fa72ff07a","max":17082925,"min":0,"orientation":"horizontal","style":"IPY_MODEL_58b33c0974924a28b6b1cf64c8eaff1a","value":17082925}},"a35e89be7b8145c6b960d84d550da75b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d2a935fa3e9a4910a1a22d6e4bf78cc2","placeholder":"​","style":"IPY_MODEL_f5bf335947d1428db321fb2b89b871d4","value":" 17.1M/17.1M [00:00&lt;00:00, 102MB/s]"}},"9693faa63a9442eeb4af842f4502bf7d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b41886a6e61e40e586d223304070d1b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f02c65cbdd84d14b12966ed421420e4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c76dad8a674b491ca2e6168fa72ff07a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58b33c0974924a28b6b1cf64c8eaff1a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d2a935fa3e9a4910a1a22d6e4bf78cc2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f5bf335947d1428db321fb2b89b871d4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bc8cdb19cffc40828e6613cf33191c2f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cfd6efe79e0844e0ad44f3515e2b214f","IPY_MODEL_687ea9095a2347cebd050b63bbcde3d9","IPY_MODEL_5c028ff31f294b90b750f6042474ea39"],"layout":"IPY_MODEL_a4c03d9016f4446d8aa467af2fab4ceb"}},"cfd6efe79e0844e0ad44f3515e2b214f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b34973a9a8f64feeb8933d5c7fd49a2d","placeholder":"​","style":"IPY_MODEL_2e06755e1d744177914e2fe4d35d6b7a","value":"special_tokens_map.json: 100%"}},"687ea9095a2347cebd050b63bbcde3d9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b4b6776f4e34ea88004a5ecb568fbbe","max":280,"min":0,"orientation":"horizontal","style":"IPY_MODEL_51e9a43f48e04c5dba59124b83a98680","value":280}},"5c028ff31f294b90b750f6042474ea39":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_31686ec49c214c15811a71e3097c0d75","placeholder":"​","style":"IPY_MODEL_a58334cb5b96452fb16183888e48f7e6","value":" 280/280 [00:00&lt;00:00, 24.9kB/s]"}},"a4c03d9016f4446d8aa467af2fab4ceb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b34973a9a8f64feeb8933d5c7fd49a2d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e06755e1d744177914e2fe4d35d6b7a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2b4b6776f4e34ea88004a5ecb568fbbe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51e9a43f48e04c5dba59124b83a98680":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"31686ec49c214c15811a71e3097c0d75":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a58334cb5b96452fb16183888e48f7e6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"01b59e4120a741f3aaf2fe666a7c69fc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bb8bdbfecd01475a92561e8a6580a07b","IPY_MODEL_ce5973a40e9640f9b9fd6ca14bf415db","IPY_MODEL_394fed2cfc054903ada727b95abe8a3d"],"layout":"IPY_MODEL_550d3547ae254892951570433ee3b23a"}},"bb8bdbfecd01475a92561e8a6580a07b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c45f8fb9a9dc411380312cc4061de057","placeholder":"​","style":"IPY_MODEL_42712d3d531d461ebcd0fe8d80c639e7","value":"config.json: 100%"}},"ce5973a40e9640f9b9fd6ca14bf415db":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6d57a4e2f8d84ed28367566f372977ab","max":982,"min":0,"orientation":"horizontal","style":"IPY_MODEL_681c381169cd42c4abdb05938c78134d","value":982}},"394fed2cfc054903ada727b95abe8a3d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c97b6ed90b24bb0951b7d3bf202df2b","placeholder":"​","style":"IPY_MODEL_bc8947ded52540bd8be18a886234652b","value":" 982/982 [00:00&lt;00:00, 35.7kB/s]"}},"550d3547ae254892951570433ee3b23a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c45f8fb9a9dc411380312cc4061de057":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42712d3d531d461ebcd0fe8d80c639e7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6d57a4e2f8d84ed28367566f372977ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"681c381169cd42c4abdb05938c78134d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9c97b6ed90b24bb0951b7d3bf202df2b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc8947ded52540bd8be18a886234652b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"959415ad37c04e8db9872f1e66e50863":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1bd46fa10c664748bddf1b7c25950b77","IPY_MODEL_d8228d2cdfa640d49edbed1775b95c9f","IPY_MODEL_e925e1d9013d4c4bbb7fe6f7ff7a5be8"],"layout":"IPY_MODEL_e646072c8259488384bc6f8b21eda948"}},"1bd46fa10c664748bddf1b7c25950b77":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_683c563504d04f899b2960fb7b962f89","placeholder":"​","style":"IPY_MODEL_7fd47ce5e33c406bbb6f9f6c96ec3346","value":"pytorch_model.bin: 100%"}},"d8228d2cdfa640d49edbed1775b95c9f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_31424171c932486592b496986107f482","max":1112257205,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c10ea5ea87bb4d669ca43341c4011737","value":1112257205}},"e925e1d9013d4c4bbb7fe6f7ff7a5be8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_68edd09a41424657b5ae2fd92b5465aa","placeholder":"​","style":"IPY_MODEL_d983b19ef5ce45119fdf0c18fb2cf95f","value":" 1.11G/1.11G [00:04&lt;00:00, 257MB/s]"}},"e646072c8259488384bc6f8b21eda948":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"683c563504d04f899b2960fb7b962f89":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7fd47ce5e33c406bbb6f9f6c96ec3346":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"31424171c932486592b496986107f482":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c10ea5ea87bb4d669ca43341c4011737":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"68edd09a41424657b5ae2fd92b5465aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d983b19ef5ce45119fdf0c18fb2cf95f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"085903dc6f494b608f008e3979b244e5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8c5acf1a53454cf9ad439fc818fd720d","IPY_MODEL_484ff09f13b6434bad895e58492ffa12","IPY_MODEL_aa5b086a7f894ddc8e41185ae6f64317"],"layout":"IPY_MODEL_9e16b5cf908c4e97918b7724b9e044d5"}},"8c5acf1a53454cf9ad439fc818fd720d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c60a635301594737ab5022aae55e8d16","placeholder":"​","style":"IPY_MODEL_c2ad8b6c14cd49ec91795df639f9fdfe","value":"model.safetensors: 100%"}},"484ff09f13b6434bad895e58492ffa12":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b8997ba9436a4da5b62848736fb791cb","max":1112212292,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b96a8a9f30f24525b3acb62037e40be4","value":1112212292}},"aa5b086a7f894ddc8e41185ae6f64317":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a54f36d87c3d4afda3dbeb3a45ee26cc","placeholder":"​","style":"IPY_MODEL_8661915c041148e49800941b95978ebf","value":" 1.11G/1.11G [00:06&lt;00:00, 112MB/s]"}},"9e16b5cf908c4e97918b7724b9e044d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c60a635301594737ab5022aae55e8d16":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2ad8b6c14cd49ec91795df639f9fdfe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b8997ba9436a4da5b62848736fb791cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b96a8a9f30f24525b3acb62037e40be4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a54f36d87c3d4afda3dbeb3a45ee26cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8661915c041148e49800941b95978ebf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"fLwfKfTYQIVq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743448132899,"user_tz":-120,"elapsed":10298,"user":{"displayName":"Alex Samoila","userId":"12167568027165400684"}},"outputId":"24c2c7f6-c44e-47c4-dff9-7423898ae2cf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.13.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","source":["# Importing the libraries needed\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","import torch\n","import seaborn as sns\n","import transformers\n","import json\n","from tqdm import tqdm\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","import logging\n","import matplotlib.pyplot as plt\n","import os\n","from datetime import datetime\n","logging.basicConfig(level=logging.ERROR)"],"metadata":{"id":"W0ZgkxVCK8_c","executionInfo":{"status":"ok","timestamp":1743448526886,"user_tz":-120,"elapsed":49,"user":{"displayName":"Alex Samoila","userId":"12167568027165400684"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# Setting up the device for GPU usage\n","\n","from torch import cuda\n","device = 'cuda' if cuda.is_available() else 'cpu'"],"metadata":{"id":"V_q1ztT4LDEh","executionInfo":{"status":"ok","timestamp":1743448175955,"user_tz":-120,"elapsed":96,"user":{"displayName":"Alex Samoila","userId":"12167568027165400684"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lj2EYCDVUHiH","executionInfo":{"status":"ok","timestamp":1743448215253,"user_tz":-120,"elapsed":20922,"user":{"displayName":"Alex Samoila","userId":"12167568027165400684"}},"outputId":"a731c653-f4d7-4607-f47e-5dbba262c78b"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd drive/MyDrive/Projects/trend_analysis"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8WChDl_sVBLg","executionInfo":{"status":"ok","timestamp":1743449850180,"user_tz":-120,"elapsed":34,"user":{"displayName":"Alex Samoila","userId":"12167568027165400684"}},"outputId":"b25ee8b6-ec0c-48e9-923d-6dc022f7eb62"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: 'drive/MyDrive/Projects/trend_analysis'\n","/content/drive/MyDrive/Projects/trend_analysis/data\n"]}]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tl7kbRYBVgaf","executionInfo":{"status":"ok","timestamp":1743449870546,"user_tz":-120,"elapsed":138,"user":{"displayName":"Alex Samoila","userId":"12167568027165400684"}},"outputId":"024e8f99-7089-4cbd-c654-e3c83c9e09f4"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["data\n"]}]},{"cell_type":"code","source":["posts_df = pd.read_json(\"data/analyzed_trends.json\")"],"metadata":{"id":"ayFy1gkXVMXB","executionInfo":{"status":"ok","timestamp":1743449886667,"user_tz":-120,"elapsed":97,"user":{"displayName":"Alex Samoila","userId":"12167568027165400684"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["posts_df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"kvjianhfVTCw","executionInfo":{"status":"ok","timestamp":1743448225373,"user_tz":-120,"elapsed":122,"user":{"displayName":"Alex Samoila","userId":"12167568027165400684"}},"outputId":"400af8c0-6f74-485a-8aef-a08a8413af6d"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                post sentiment  label\n","0  Annonce ✨Vind smykker for i alt 10.000 kr.✨\\n\\...  positive      2\n","1  reklame @ginatricot | Vil du hjælpe mig med, h...  positive      2\n","2  annonce | Mangler du et julegaveønske? 🫶\\nLige...  positive      2\n","3  Vind 2x valgfrie smykker & 1x smykkeæske fra P...  positive      2\n","4  annonce Glædelig 3. søndag i advent! 🥰🎄✨\\n\\nI ...  positive      2"],"text/html":["\n","  <div id=\"df-e3d29bcc-3627-4107-a4cf-bba57799ad6b\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>post</th>\n","      <th>sentiment</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Annonce ✨Vind smykker for i alt 10.000 kr.✨\\n\\...</td>\n","      <td>positive</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>reklame @ginatricot | Vil du hjælpe mig med, h...</td>\n","      <td>positive</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>annonce | Mangler du et julegaveønske? 🫶\\nLige...</td>\n","      <td>positive</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Vind 2x valgfrie smykker &amp; 1x smykkeæske fra P...</td>\n","      <td>positive</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>annonce Glædelig 3. søndag i advent! 🥰🎄✨\\n\\nI ...</td>\n","      <td>positive</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e3d29bcc-3627-4107-a4cf-bba57799ad6b')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-e3d29bcc-3627-4107-a4cf-bba57799ad6b button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-e3d29bcc-3627-4107-a4cf-bba57799ad6b');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-1b376cfa-531e-43ca-8910-ff838bb6fe3e\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1b376cfa-531e-43ca-8910-ff838bb6fe3e')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-1b376cfa-531e-43ca-8910-ff838bb6fe3e button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"posts_df","summary":"{\n  \"name\": \"posts_df\",\n  \"rows\": 5178,\n  \"fields\": [\n    {\n      \"column\": \"post\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2911,\n        \"samples\": [\n          \"Kender I? \\ud83e\\udd72 Selvom jeg er n\\u00e5et langt i mit forhold til mad, krop og tr\\u00e6ning, s\\u00e5 kan jeg stadig have tanker som disse, n\\u00e5r jeg er \\\"fastl\\u00e5st\\\" til sofaen under sygdom \\ud83e\\udd27\\n\\nJeg er dog heldigvis blevet bedre til at \\\"hoppe p\\u00e5\\\" og dyrke de fornuftige tanker, som er gode for min krop og mit helbred \\u2014 og vinke farvel til de andre tanker \\ud83d\\udc4b\\ud83c\\udffc \\n\\nFor ved I hvad? Det vigtigste er, at vi er gode ved vores os selv. Det er vores krop og vores helbred, som skal b\\u00e6re os gennem livet \\u2728 Tr\\u00e6ning, aktivitet og andre g\\u00f8rem\\u00e5l l\\u00f8ber ingen vegne og st\\u00e5r klar, n\\u00e5r vi igen er kommet p\\u00e5 toppen \\ud83d\\ude0c\\ud83e\\ude77\",\n          \"F\\u00f8rst og fremmest, s\\u00e5 har jeg bare lige brug for at sige tak til jer. Af hjertet tak for alle de beskeder jeg fik ig\\u00e5r, de reddede mig virkelig \\ud83d\\udc94\\ud83d\\udc94 jeg sad virkelig med en f\\u00f8lelse af at jeg blev kvalt.. Mit hjerte bankede s\\u00e5 hurtigt at jeg ikke kunne styre det og jeg endte med at g\\u00e5 rundt i lejligheden imens jeg rystede helt ustyrligt.. Heldigvis kom min mor og jeg fik drukket en masse vand og snakket om nogle ting der kunne f\\u00e5 mine tanker hen p\\u00e5 noget andet.. Det hjalp virkelig meget! Men den helt store grund til at jeg er okay, er virkelig jeres skyld!! Jer der tog jer tid til at skrive til mig \\ud83d\\ude2d I gav mig s\\u00e5 mange gode r\\u00e5d og s\\u00f8de ord, som virkelig ramte mig, p\\u00e5 den gode m\\u00e5de! Jeres r\\u00e5d, som endte med at f\\u00e5 mig til at blive okay igen, dem har jeg skrevet til sidst i videoen, til hvis nogen af jer en dag f\\u00e5r brug for dem og sidder i samme situation! Jeg elsker jer af hele mit hjerte. Tak fordi i var der for mig! Det betyder alt i hele verden \\ud83d\\ude2d\\ud83d\\ude2d\\ud83d\\ude2d\\u2665\\ufe0f\\u2665\\ufe0f\\u2665\\ufe0f\",\n          \"my makeup routine\\ud83e\\udebd\\ud83e\\udebd #makeup \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"positive\",\n          \"neutral\",\n          \"negative\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2,\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["# Defining some key variables that will be used later on in the training\n","MAX_LEN = 256\n","TRAIN_BATCH_SIZE = 8\n","VALID_BATCH_SIZE = 4\n","TEST_BATCH_SIZE = 4\n","# EPOCHS = 1\n","LEARNING_RATE = 1e-05\n","tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual\", truncation=True, do_lower_case=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":269,"referenced_widgets":["46a298f602854d1fb1c381c689a5fa27","0bc2aae051ff41229193bd6ddc359644","b317ac03cd9545c2b5f860dab3d3e5e8","20dcea837a1a4cd898f50491e836e958","50e8e19a68d24d6eaecbbe4ca83c7c65","b79787e63854401486086d1b6adbaaaf","08c602c3897041b795702e3f4bd798a3","354842de0ab443e8828ff975a99f72f2","324d0c6787e04ce0ae0012e45ce0488e","4789e69093f047da8b59e8f3f83e95b6","00ea35acadad44afadd2835ffff03824","ed56f740eb4041afa20a9bb0cfb92062","168795b00a8e4ee9b9c54aeba51ce5f9","c9e4e7c345d547ca81f79dd115a4a504","dc916c794cbf49ce8e9dae0c4d4313ce","ea3858e9182a498fbef0532e2d0e7bac","92d485a2fa254dcdb5061d6da735877d","a5c17baae3414ac58a8698b2219500a1","2906efda23874955b1043ce0ada1c2c9","ffbee74e38af43b0938a2792f8fd136e","1fd38e10747447538d74735ab53b61ce","f97d5fcb240349f788b646078c0aaae4","bd430ce15d3f4a63b35eed4ff1cc8ed9","3b69f42a75cc481ebdc63626c81f596b","7e35f6ce168b4749a0e6639d5d2db430","a35e89be7b8145c6b960d84d550da75b","9693faa63a9442eeb4af842f4502bf7d","b41886a6e61e40e586d223304070d1b9","1f02c65cbdd84d14b12966ed421420e4","c76dad8a674b491ca2e6168fa72ff07a","58b33c0974924a28b6b1cf64c8eaff1a","d2a935fa3e9a4910a1a22d6e4bf78cc2","f5bf335947d1428db321fb2b89b871d4","bc8cdb19cffc40828e6613cf33191c2f","cfd6efe79e0844e0ad44f3515e2b214f","687ea9095a2347cebd050b63bbcde3d9","5c028ff31f294b90b750f6042474ea39","a4c03d9016f4446d8aa467af2fab4ceb","b34973a9a8f64feeb8933d5c7fd49a2d","2e06755e1d744177914e2fe4d35d6b7a","2b4b6776f4e34ea88004a5ecb568fbbe","51e9a43f48e04c5dba59124b83a98680","31686ec49c214c15811a71e3097c0d75","a58334cb5b96452fb16183888e48f7e6"]},"id":"sRHrAE_8Yp0f","executionInfo":{"status":"ok","timestamp":1743448231496,"user_tz":-120,"elapsed":4589,"user":{"displayName":"Alex Samoila","userId":"12167568027165400684"}},"outputId":"417d4e8f-2496-4c5e-8a62-64259404f641"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46a298f602854d1fb1c381c689a5fa27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed56f740eb4041afa20a9bb0cfb92062"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd430ce15d3f4a63b35eed4ff1cc8ed9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc8cdb19cffc40828e6613cf33191c2f"}},"metadata":{}}]},{"cell_type":"code","source":["class SentimentData(Dataset):\n","    def __init__(self, dataframe, tokenizer, max_len):\n","        self.tokenizer = tokenizer\n","        self.data = dataframe\n","        self.text = dataframe.post\n","        self.targets = self.data.label\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.text)\n","\n","    def __getitem__(self, index):\n","        # Get the actual row using iloc to avoid index issues\n","        row = self.data.iloc[index]  # Use iloc to access by position\n","        text = str(row.post)\n","        text = \" \".join(text.split())\n","\n","        inputs = self.tokenizer.encode_plus(\n","            text,\n","            None,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            pad_to_max_length=True,\n","            return_token_type_ids=True\n","        )\n","        ids = inputs['input_ids']\n","        mask = inputs['attention_mask']\n","        token_type_ids = inputs[\"token_type_ids\"]\n","\n","        return {\n","            'ids': torch.tensor(ids, dtype=torch.long),\n","            'mask': torch.tensor(mask, dtype=torch.long),\n","            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n","            'targets': torch.tensor(row.label, dtype=torch.float) # Access label from the row\n","        }"],"metadata":{"id":"X44K-GfbYqZg","executionInfo":{"status":"ok","timestamp":1743449012790,"user_tz":-120,"elapsed":38,"user":{"displayName":"Alex Samoila","userId":"12167568027165400684"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["train, validate, test = np.split(\n","        posts_df.sample(frac=1, random_state=42),\n","        [int(0.8 * len(posts_df)), int(0.9 * len(posts_df))],\n","    )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2KI_Q0fiYOkJ","executionInfo":{"status":"ok","timestamp":1743449018027,"user_tz":-120,"elapsed":23,"user":{"displayName":"Alex Samoila","userId":"12167568027165400684"}},"outputId":"c94ba38d-26e6-4d17-de61-16187a28e0d8"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/numpy/_core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n","  return bound(*args, **kwds)\n"]}]},{"cell_type":"code","source":["print(\"Train, Validate, Test:\",train.shape, validate.shape, test.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8_DZE85UZU0L","executionInfo":{"status":"ok","timestamp":1743449020718,"user_tz":-120,"elapsed":43,"user":{"displayName":"Alex Samoila","userId":"12167568027165400684"}},"outputId":"7f6e4f9e-bc33-4983-8458-347b20f4d49e"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Train, Validate, Test: (4142, 3) (518, 3) (518, 3)\n"]}]},{"cell_type":"code","source":["training_set = SentimentData(train, tokenizer, MAX_LEN)\n","validation_set = SentimentData(validate, tokenizer, MAX_LEN)\n","test_set = SentimentData(test, tokenizer, MAX_LEN)\n","train_params = {'batch_size': TRAIN_BATCH_SIZE,\n","                'shuffle': True,\n","                'num_workers': 1\n","                }\n","valid_params = {'batch_size': VALID_BATCH_SIZE,\n","                'shuffle': True,\n","                'num_workers': 1\n","                }\n","test_params = {'batch_size': TEST_BATCH_SIZE,\n","                'shuffle': True,\n","                'num_workers': 1\n","                }\n","training_loader = DataLoader(training_set, **train_params)\n","validation_loader = DataLoader(validation_set, **valid_params)\n","test_loader = DataLoader(test_set, **test_params)"],"metadata":{"id":"QxFLwKtZZnZQ","executionInfo":{"status":"ok","timestamp":1743449021929,"user_tz":-120,"elapsed":2,"user":{"displayName":"Alex Samoila","userId":"12167568027165400684"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual\")\n","model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":877,"referenced_widgets":["01b59e4120a741f3aaf2fe666a7c69fc","bb8bdbfecd01475a92561e8a6580a07b","ce5973a40e9640f9b9fd6ca14bf415db","394fed2cfc054903ada727b95abe8a3d","550d3547ae254892951570433ee3b23a","c45f8fb9a9dc411380312cc4061de057","42712d3d531d461ebcd0fe8d80c639e7","6d57a4e2f8d84ed28367566f372977ab","681c381169cd42c4abdb05938c78134d","9c97b6ed90b24bb0951b7d3bf202df2b","bc8947ded52540bd8be18a886234652b","959415ad37c04e8db9872f1e66e50863","1bd46fa10c664748bddf1b7c25950b77","d8228d2cdfa640d49edbed1775b95c9f","e925e1d9013d4c4bbb7fe6f7ff7a5be8","e646072c8259488384bc6f8b21eda948","683c563504d04f899b2960fb7b962f89","7fd47ce5e33c406bbb6f9f6c96ec3346","31424171c932486592b496986107f482","c10ea5ea87bb4d669ca43341c4011737","68edd09a41424657b5ae2fd92b5465aa","d983b19ef5ce45119fdf0c18fb2cf95f","085903dc6f494b608f008e3979b244e5","8c5acf1a53454cf9ad439fc818fd720d","484ff09f13b6434bad895e58492ffa12","aa5b086a7f894ddc8e41185ae6f64317","9e16b5cf908c4e97918b7724b9e044d5","c60a635301594737ab5022aae55e8d16","c2ad8b6c14cd49ec91795df639f9fdfe","b8997ba9436a4da5b62848736fb791cb","b96a8a9f30f24525b3acb62037e40be4","a54f36d87c3d4afda3dbeb3a45ee26cc","8661915c041148e49800941b95978ebf"]},"id":"S9w4sVGobJYB","executionInfo":{"status":"ok","timestamp":1743448272598,"user_tz":-120,"elapsed":11064,"user":{"displayName":"Alex Samoila","userId":"12167568027165400684"}},"outputId":"71eb9357-b050-4ed7-f76b-18abd571fc7c"},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/982 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01b59e4120a741f3aaf2fe666a7c69fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"959415ad37c04e8db9872f1e66e50863"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"085903dc6f494b608f008e3979b244e5"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["XLMRobertaForSequenceClassification(\n","  (roberta): XLMRobertaModel(\n","    (embeddings): XLMRobertaEmbeddings(\n","      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): XLMRobertaEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x XLMRobertaLayer(\n","          (attention): XLMRobertaAttention(\n","            (self): XLMRobertaSdpaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): XLMRobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): XLMRobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): XLMRobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): XLMRobertaClassificationHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["def calcuate_accuracy(preds, targets):\n","    n_correct = (preds==targets).sum().item()\n","    return n_correct\n","def one_epoch(model, data_loader, loss_fn, opt=None):\n","    \"\"\"\n","    Runs one epoch of training or validation.\n","\n","    Args:\n","        model: The model to be trained or evaluated.\n","        data_loader: DataLoader object that loads the batch of data.\n","        loss_fn: The loss function to optimize.\n","        opt: Optimizer for training phase (None during evaluation).\n","\n","    Returns:\n","        avg_loss: Average loss over the epoch.\n","    \"\"\"\n","\n","    # Set the model to training or evaluation mode based on the optimizer\n","    train = False if opt is None else True\n","    model.train() if train else model.eval()\n","    tr_loss = 0\n","    n_correct = 0\n","    nb_tr_steps = 0\n","    nb_tr_examples = 0\n","\n","    losses = []\n","    for _,data in enumerate(data_loader):\n","        ids = data['ids'].to(device, dtype = torch.long)\n","        mask = data['mask'].to(device, dtype = torch.long)\n","        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n","        targets = data['targets'].to(device, dtype = torch.long)\n","        # Enable gradient computation only during training\n","        with torch.set_grad_enabled(train):\n","            outputs = model(ids, mask, token_type_ids).logits\n","        loss = loss_fn(outputs, targets)\n","        tr_loss += loss.item()\n","        big_val, big_idx = torch.max(outputs.data, dim=1)\n","        n_correct += calcuate_accuracy(big_idx, targets)\n","\n","        nb_tr_steps += 1\n","        nb_tr_examples+=targets.size(0)\n","\n","        if _%5000==0:\n","            loss_step = tr_loss/nb_tr_steps\n","            accu_step = (n_correct*100)/nb_tr_examples\n","            print(f\"Training Loss per 5000 steps: {loss_step}\")\n","            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n","\n","        if train:\n","            opt.zero_grad()  # Reset gradients\n","            loss.backward()  # Backpropagate gradients\n","            opt.step()  # Update model parameters\n","            # if scheduler:\n","            #     scheduler.step()\n","\n","        losses.append(loss.item())\n","\n","    avg_loss = np.mean(losses)\n","    print(f'The Total Accuracy for Epoch: {(n_correct*100)/nb_tr_examples}')\n","    epoch_loss = tr_loss/nb_tr_steps\n","    epoch_accu = (n_correct*100)/nb_tr_examples\n","    print(f\"Training Loss Epoch: {epoch_loss}\")\n","    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n","    return avg_loss\n","\n","def train(\n","    model,\n","    train_loader,\n","    val_loader,\n","    lr=1e-3,\n","    max_epochs=3,\n","    patience=3,\n","):\n","    \"\"\"\n","    Trains the model using the provided training and validation data loaders.\n","\n","    Args:\n","        model: The neural network model to train.\n","        train_loader: DataLoader object for the training set.\n","        val_loader: DataLoader object for the validation set.\n","        lr: Learning rate for the optimizer. Default is 1e-3.\n","        max_epochs: Maximum number of epochs to train. Default is 30.\n","        weight_decay: Weight decay for L2 regularization. Default is 0.01.\n","        patience: Number of epochs with no improvement after which training will stop. Default is 3.\n","        custom_loss: Custom loss function (if needed). Default is None.\n","\n","    Returns:\n","        train_losses: List of training losses per epoch.\n","        valid_losses: List of validation losses per epoch.\n","    \"\"\"\n","\n","    # Initialize the optimizer\n","    opt = torch.optim.Adam(model.parameters(), lr=lr)\n","\n","    # Add learning rate scheduler\n","    # scheduler = torch.optim.lr_scheduler.OneCycleLR(\n","    #     opt,\n","    #     max_lr=lr,\n","    #     epochs=max_epochs,\n","    #     steps_per_epoch=len(train_loader),\n","    #     pct_start=0.3,  # Spend 30% of time warming up\n","    #     div_factor=25.0,  # Start with lr/25\n","    #     final_div_factor=1e4,  # End with lr/10000\n","    # )\n","\n","    loss_fn = torch.nn.CrossEntropyLoss()\n","    best_valid_loss = float(\"inf\")  # Initialize best validation loss\n","    patience_counter = 0  # Counter for early stopping\n","    train_losses, valid_losses = [], []  # Track losses for plotting/analysis\n","\n","    # Training loop\n","    t = tqdm(range(max_epochs))\n","    for epoch in t:\n","        # Training phase\n","        train_loss = one_epoch(model, train_loader, loss_fn, opt)\n","        # Validation phase\n","        valid_loss = one_epoch(model, val_loader, loss_fn)\n","\n","        train_losses.append(train_loss)\n","        valid_losses.append(valid_loss)\n","\n","        # Early stopping\n","        if valid_loss < best_valid_loss:\n","            best_valid_loss = valid_loss\n","            patience_counter = 0  # Reset counter if validation loss improves\n","        else:\n","            patience_counter += 1\n","            if patience_counter >= patience:\n","                print(f\"\\nEarly stopping at epoch {epoch}\")\n","                break\n","\n","        t.set_description(f\"train loss: {train_loss:.4f}, val loss: {valid_loss:.4f}\")\n","\n","    return train_losses, valid_losses\n","\n","def plot_history(train_losses, valid_losses):\n","    \"\"\"\n","    Plots the training and validation loss history over epochs.\n","\n","    Args:\n","        train_losses: List of training losses for each epoch.\n","        valid_losses: List of validation losses for each epoch.\n","    \"\"\"\n","    plt.figure(figsize=(7, 3))\n","    plt.subplot(1, 2, 1)\n","    plt.xlabel(\"epoch\")\n","    plt.ylabel(\"loss\")\n","    plt.plot(train_losses, label=\"train\")\n","    plt.plot(valid_losses, label=\"valid\")\n","    plt.legend()\n","    plt.grid()\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"id":"I1jvCagVcPgU","executionInfo":{"status":"ok","timestamp":1743449075320,"user_tz":-120,"elapsed":55,"user":{"displayName":"Alex Samoila","userId":"12167568027165400684"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["EPOCHS = 5\n","PATIENCE = 2\n","\n","# Start the training process and plot the training history\n","print(\"Begin training ...\")\n","plot_history(\n","    *train(\n","        model,\n","        training_loader,\n","        validation_loader,\n","        lr=LEARNING_RATE,\n","        max_epochs=EPOCHS,\n","        patience=PATIENCE,\n","    )\n",")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"eAsh3RrBibkv","executionInfo":{"status":"ok","timestamp":1743453396734,"user_tz":-120,"elapsed":1106158,"user":{"displayName":"Alex Samoila","userId":"12167568027165400684"}},"outputId":"b8cfabb8-7703-490b-b300-ae7bb28342cf"},"execution_count":73,"outputs":[{"output_type":"stream","name":"stdout","text":["Begin training ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/5 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2700: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss per 5000 steps: 0.20849649608135223\n","Training Accuracy per 5000 steps: 87.5\n","The Total Accuracy for Epoch: 94.97827136648962\n","Training Loss Epoch: 0.1471782112566028\n","Training Accuracy Epoch: 94.97827136648962\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2700: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss per 5000 steps: 0.0033059963025152683\n","Training Accuracy per 5000 steps: 100.0\n"]},{"output_type":"stream","name":"stderr","text":["train loss: 0.1472, val loss: 0.2644:  20%|██        | 1/5 [03:41<14:46, 221.58s/it]"]},{"output_type":"stream","name":"stdout","text":["The Total Accuracy for Epoch: 91.6988416988417\n","Training Loss Epoch: 0.2643762280824236\n","Training Accuracy Epoch: 91.6988416988417\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2700: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss per 5000 steps: 0.00992458313703537\n","Training Accuracy per 5000 steps: 100.0\n","The Total Accuracy for Epoch: 96.20956059874457\n","Training Loss Epoch: 0.11155731289047015\n","Training Accuracy Epoch: 96.20956059874457\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2700: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss per 5000 steps: 0.0011426261626183987\n","Training Accuracy per 5000 steps: 100.0\n"]},{"output_type":"stream","name":"stderr","text":["train loss: 0.1116, val loss: 0.2621:  40%|████      | 2/5 [07:22<11:03, 221.26s/it]"]},{"output_type":"stream","name":"stdout","text":["The Total Accuracy for Epoch: 91.89189189189189\n","Training Loss Epoch: 0.2620571052745128\n","Training Accuracy Epoch: 91.89189189189189\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2700: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss per 5000 steps: 0.008414089679718018\n","Training Accuracy per 5000 steps: 100.0\n","The Total Accuracy for Epoch: 96.61999034282955\n","Training Loss Epoch: 0.09911806263212963\n","Training Accuracy Epoch: 96.61999034282955\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2700: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss per 5000 steps: 0.003692375961691141\n","Training Accuracy per 5000 steps: 100.0\n"]},{"output_type":"stream","name":"stderr","text":["train loss: 0.0991, val loss: 0.2524:  60%|██████    | 3/5 [11:03<07:22, 221.23s/it]"]},{"output_type":"stream","name":"stdout","text":["The Total Accuracy for Epoch: 92.08494208494209\n","Training Loss Epoch: 0.2524171556059558\n","Training Accuracy Epoch: 92.08494208494209\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2700: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss per 5000 steps: 0.011033846065402031\n","Training Accuracy per 5000 steps: 100.0\n","The Total Accuracy for Epoch: 97.03042008691453\n","Training Loss Epoch: 0.08034629606853984\n","Training Accuracy Epoch: 97.03042008691453\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2700: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss per 5000 steps: 1.312369704246521\n","Training Accuracy per 5000 steps: 75.0\n"]},{"output_type":"stream","name":"stderr","text":["train loss: 0.0803, val loss: 0.3009:  80%|████████  | 4/5 [14:44<03:41, 221.20s/it]"]},{"output_type":"stream","name":"stdout","text":["The Total Accuracy for Epoch: 92.66409266409266\n","Training Loss Epoch: 0.3009013762039383\n","Training Accuracy Epoch: 92.66409266409266\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2700: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss per 5000 steps: 0.0032315393909811974\n","Training Accuracy per 5000 steps: 100.0\n","The Total Accuracy for Epoch: 97.68227909222598\n","Training Loss Epoch: 0.06665887741714044\n","Training Accuracy Epoch: 97.68227909222598\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2700: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss per 5000 steps: 0.004683507606387138\n","Training Accuracy per 5000 steps: 100.0\n"]},{"output_type":"stream","name":"stderr","text":["\rtrain loss: 0.0803, val loss: 0.3009:  80%|████████  | 4/5 [18:25<04:36, 276.49s/it]"]},{"output_type":"stream","name":"stdout","text":["The Total Accuracy for Epoch: 92.66409266409266\n","Training Loss Epoch: 0.2961059716583874\n","Training Accuracy Epoch: 92.66409266409266\n","\n","Early stopping at epoch 4\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 700x300 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["# Save the trained model and its hyperparameters\n","timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n","# Folder name to save the trained model\n","models_dir = \"checkpoints\"\n","os.makedirs(models_dir, exist_ok=True)\n","\n","# Create model name with hyperparameters\n","model_name = \"roberta_sentiment\"\n","\n","# Construct the path for saving the model\n","save_dir = os.path.join(models_dir, f\"{timestamp}_{model_name}\")\n","os.makedirs(save_dir, exist_ok=True)\n","model_path = os.path.join(save_dir, \"model.bin\")\n","model.save_pretrained(save_dir)\n","# torch.save(model.state_dict(), model_path)\n","tokenizer.save_vocabulary(save_dir)\n","print(\"Model saved!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PWZGk9Ewspx9","executionInfo":{"status":"ok","timestamp":1743453852288,"user_tz":-120,"elapsed":7200,"user":{"displayName":"Alex Samoila","userId":"12167568027165400684"}},"outputId":"a3dad4dd-1849-4008-8efd-8113f5cf184c"},"execution_count":74,"outputs":[{"output_type":"stream","name":"stdout","text":["Model saved!\n"]}]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jbkeFuMPwWR7","executionInfo":{"status":"ok","timestamp":1743451107157,"user_tz":-120,"elapsed":114,"user":{"displayName":"Alex Samoila","userId":"12167568027165400684"}},"outputId":"4a9fa896-e6b7-41df-b705-0724d42f2688"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["checkpoints  data\n"]}]},{"cell_type":"code","source":["test_model = AutoModelForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/Projects/trend_analysis/checkpoints/20250331_2044_roberta_sentiment\")\n","# checkpoint = torch.load(\"/content/drive/MyDrive/Projects/trend_analysis/checkpoints/20250331_1958_roberta_sentiment/model.bin\", weights_only=False)\n","# test_model.load_state_dict(checkpoint)\n","test_model.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e31D9VsfuI1D","executionInfo":{"status":"ok","timestamp":1743453863306,"user_tz":-120,"elapsed":705,"user":{"displayName":"Alex Samoila","userId":"12167568027165400684"}},"outputId":"37ae7d20-2ede-461a-82e9-48f2fdeaf58c"},"execution_count":75,"outputs":[{"output_type":"execute_result","data":{"text/plain":["XLMRobertaForSequenceClassification(\n","  (roberta): XLMRobertaModel(\n","    (embeddings): XLMRobertaEmbeddings(\n","      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): XLMRobertaEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x XLMRobertaLayer(\n","          (attention): XLMRobertaAttention(\n","            (self): XLMRobertaSdpaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): XLMRobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): XLMRobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): XLMRobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): XLMRobertaClassificationHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":75}]},{"cell_type":"code","source":["def evaluate_model(model, data_loader):\n","    n_correct = 0\n","    nb_tr_examples = 0\n","    all_predictions = []\n","    all_true_ratings = []\n","\n","    with torch.no_grad():\n","      for _,data in enumerate(data_loader):\n","          ids = data['ids'].to(device, dtype = torch.long)\n","          mask = data['mask'].to(device, dtype = torch.long)\n","          token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n","          targets = data['targets'].to(device, dtype = torch.long)\n","\n","          outputs = model(ids, mask, token_type_ids).logits\n","\n","          big_val, big_idx = torch.max(outputs.data, dim=1)\n","          n_correct += calcuate_accuracy(big_idx, targets)\n","          nb_tr_examples+=targets.size(0)\n","\n","\n","    accu_step = (n_correct*100)/nb_tr_examples\n","\n","    return accu_step"],"metadata":{"id":"nuSjt_-dyfNz","executionInfo":{"status":"ok","timestamp":1743453870615,"user_tz":-120,"elapsed":51,"user":{"displayName":"Alex Samoila","userId":"12167568027165400684"}}},"execution_count":76,"outputs":[]},{"cell_type":"code","source":["test_model.to(device)\n","evaluate_model(test_model, test_loader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wqs7bU93z_WH","executionInfo":{"status":"ok","timestamp":1743453924178,"user_tz":-120,"elapsed":7623,"user":{"displayName":"Alex Samoila","userId":"12167568027165400684"}},"outputId":"73cffc6f-db71-4c1b-8b60-c3e852c40b72"},"execution_count":79,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2700: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["91.31274131274131"]},"metadata":{},"execution_count":79}]},{"cell_type":"code","source":["init_model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual\")\n","init_model.to(device)\n","init_model.eval()\n","print(evaluate_model(init_model, test_loader))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z_iC5EgO0wPj","executionInfo":{"status":"ok","timestamp":1743453951871,"user_tz":-120,"elapsed":9970,"user":{"displayName":"Alex Samoila","userId":"12167568027165400684"}},"outputId":"12413ce1-fc6a-4eb2-8d51-d62f0559d581"},"execution_count":81,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2700: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["63.12741312741313\n"]}]}]}